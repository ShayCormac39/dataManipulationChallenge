---
title: "Data Manipulation Challenge"
subtitle: "A Mental Model for Method Chaining in Pandas"
format:
  html: default
execute:
  echo: true
  eval: true
---

# üîó Data Manipulation Challenge - A Mental Model for Method Chaining in Pandas

::: {.callout-important}
## üìä Challenge Requirements In Section [Student Analysis Section](#student-analysis-section)
- Complete discussion questions for at least 5 of the 7 mental models (for higher grades, complete all 7 plus additional requirements ‚Äî see [Grading Rubric](#grading-rubric) for details)
:::

## Getting Started: Repository Setup üöÄ

::: {.callout-important}
## üìÅ Getting Started

**Step 1:** Fork the challenge repository to your own GitHub account

- Go to [https://github.com/flyaflya/dataManipulationChallenge](https://github.com/flyaflya/dataManipulationChallenge)
- Click the **"Fork"** button (top-right corner) to create your own copy of the repository under your GitHub account
- **Important:** You must fork first so that you have your own repository to push changes to and deploy via GitHub Pages

**Step 2:** Clone **your fork** (not the original) to your local machine

- Navigate to **your forked repository** on GitHub (it will be at `https://github.com/YOUR-USERNAME/dataManipulationChallenge`)
- Click the green **"<> Code"** button on **your fork's** GitHub page
- Copy the URL (HTTPS is recommended)
- Open Cursor, open a terminal, and run: `git clone <paste-your-URL-here>`
- Open the cloned folder in Cursor

**Step 3:** Set up your Python environment (see [Python Setup](#note-on-python-usage) section below)

**Step 4:** You're ready to start! The data loading code is already provided in this file.

**Note:** This challenge uses the same `index.qmd` file you're reading right now ‚Äî you'll edit it to complete your analysis.
:::

::: {.callout-warning}
## üíæ Important: Save Your Work Frequently!

**Before you start:** Make sure to commit your work often using the Source Control panel in Cursor (Ctrl+Shift+G or Cmd+Shift+G). This prevents the AI from overwriting your progress and ensures you don't lose your work.

**Commit after each major step:**

- After completing each mental model section
- After adding your visualizations
- After completing your advanced method chain
- Before asking the AI for help with new code

**How to commit:**

1. Open Source Control panel (Ctrl+Shift+G)
2. Stage your changes (+ button)
3. Write a descriptive commit message
4. Click the checkmark to commit

*Remember: Frequent commits are your safety net!*
:::

::: {.callout-important}
## üéØ Note on Python Usage {#note-on-python-usage}

**Recommended Workflow: Use Your Existing Virtual Environment**
If you completed the Tech Setup Challenge Part 2, you already have a virtual environment set up! Here's how to use it for this new challenge:

1. **Fork and clone this challenge repository** (see [Getting Started](#getting-started-repository-setup-) section above for the link and detailed instructions)
2. **Open the cloned repository in Cursor**
3. **Set this project to use your existing Python interpreter:**
   - Press `Ctrl+Shift+P` ‚Üí "Python: Select Interpreter"
   - Navigate to and choose the interpreter from your existing virtual environment (e.g., `your-previous-project/venv/Scripts/python.exe`)
4. **Activate the environment in your terminal:**
   - Open terminal in Cursor (`Ctrl + ``)
   - Navigate to your previous project folder where you have the `venv` folder
   - **üí° Pro tip:** You can quickly navigate by typing `cd` followed by dragging the folder from your file explorer into the terminal
   - Activate using the appropriate command for your system:
     - **Windows Command Prompt:** `venv\Scripts\activate`
     - **Windows PowerShell:** `.\venv\Scripts\Activate.ps1`
     - **Mac/Linux:** `source venv/bin/activate`
   - You should see `(venv)` at the beginning of your terminal prompt
5. **Install additional packages if needed:** `pip install pandas numpy matplotlib seaborn`

::: {.callout-warning}
## ‚ö†Ô∏è Cloud Storage Warning

**Avoid using Google Drive, OneDrive, or other cloud storage for Python projects!** These services can cause issues with:
- Package installations failing due to file locking
- Virtual environment corruption
- Slow performance during pip operations

**Best practice:** Keep your Python projects in a local folder like `C:\Users\YourName\Documents\` or `~/Documents/` instead of cloud-synced folders.
:::

**Alternative: Create a New Virtual Environment**
If you prefer a fresh environment, follow the Quarto documentation: [https://quarto.org/docs/projects/virtual-environments.html](https://quarto.org/docs/projects/virtual-environments.html). Be sure to follow the instructions to activate the environment, set it up as your default Python interpreter for the project, and install the necessary packages (e.g. pandas) for this challenge.  For installing the packages, you can use the `pip install -r requirements.txt` command since you already have the requirements.txt file in your project.   Some steps do take a bit of time, so be patient.

**Why This Works:** Virtual environments are portable - you can use the same environment across multiple projects, and Cursor automatically activates it when you select the interpreter!

:::

## The Problem: Mastering Data Manipulation Through Method Chaining

**Core Question:** How can we efficiently manipulate datasets using `pandas` method chaining to answer complex business questions?

**The Challenge:** Real-world data analysis requires combining multiple data manipulation techniques in sequence. Rather than creating intermediate variables at each step, method chaining allows us to write clean, readable code that flows logically from one operation to the next.

**Our Approach:** We'll work with ZappTech's shipment data to answer critical business questions about service levels and cross-category orders, using the seven mental models of data manipulation through pandas method chaining.

::: {.callout-warning}
## ‚ö†Ô∏è AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance ‚Üí Awareness ‚Üí Learning ‚Üí Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## The Seven Mental Models of Data Manipulation

The seven most important ways we manipulate datasets are:

1. **Assign:** Add new variables with calculations and transformations
2. **Subset:** Filter data based on conditions or select specific columns
3. **Drop:** Remove unwanted variables or observations
4. **Sort:** Arrange data by values or indices
5. **Aggregate:** Summarize data using functions like mean, sum, count
6. **Merge:** Combine information from multiple datasets
7. **Split-Apply-Combine:** Group data and apply functions within groups


## Data and Business Context

We analyze ZappTech's shipment data, which contains information about product deliveries across multiple categories. This dataset is ideal for our analysis because:

- **Real Business Questions:** CEO wants to understand service levels and cross-category shopping patterns
- **Multiple Data Sources:** Requires merging shipment data with product category information
- **Complex Relationships:** Service levels may vary by product category, and customers may order across categories
- **Method Chaining Practice:** Perfect for demonstrating all seven mental models in sequence

## Data Loading and Initial Exploration

Let's start by loading the ZappTech shipment data and understanding what we're working with.

```{python}
#| label: load-data
#| echo: true
#| message: false
#| warning: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

# Load the shipment data
shipments_df = pd.read_csv(
    "https://raw.githubusercontent.com/flyaflya/persuasive/main/shipments.csv", 
    parse_dates=['plannedShipDate', 'actualShipDate']
)

# Load product line data
product_line_df = pd.read_csv(
    "https://raw.githubusercontent.com/flyaflya/persuasive/main/productLine.csv"
)

# Reduce dataset size for faster processing (4,000 rows instead of 96,805 rows)
shipments_df = shipments_df.head(4000)

print("Shipments data shape:", shipments_df.shape)
print("\nShipments data columns:", shipments_df.columns.tolist())
print("\nFirst few rows of shipments data:")
print(shipments_df.head(10))

print("\n" + "="*50)
print("Product line data shape:", product_line_df.shape)
print("\nProduct line data columns:", product_line_df.columns.tolist())
print("\nFirst few rows of product line data:")
print(product_line_df.head(10))
```

::: {.callout-note}
## üí° Understanding the Data

**Shipments Data:** Contains individual line items for each shipment, including:
- `shipID`: Unique identifier for each shipment
- `partID`: Product identifier
- `plannedShipDate`: When the shipment was supposed to go out
- `actualShipDate`: When it actually shipped
- `quantity`: How many units were shipped

**Product Category and Line Data:** Contains product category information:
- `partID`: Links to shipments data
- `productLine`: The specific product line within a category
- `prodCategory`: The broader category each product belongs to

**Business Questions We'll Answer:**
1. Does service level (on-time shipments) vary across product categories?
2. How often do orders include products from more than one category?
:::

## The Seven Mental Models: A Progressive Learning Journey

::: {.callout-note}
## üéØ Method Chaining Philosophy

> "Each operation should build naturally on the previous one"

*Think of method chaining like building with LEGO blocks - each piece connects to the next, creating something more complex and useful than the individual pieces.*
:::

Now we'll work through each of the seven mental models using method chaining, starting simple and building complexity.

### 1. Assign: Adding New Variables

**Mental Model:** Create new columns with calculations and transformations.

Let's start by calculating whether each shipment was late:

```{python}
#| label: mental-model-1-assign
#| echo: true

# Simple assignment - calculate if shipment was late
shipments_with_lateness = (
    shipments_df
    .assign(
        is_late=lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late=lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days
    )
)

print("Added lateness calculations:")
print(shipments_with_lateness[['shipID', 'plannedShipDate', 'actualShipDate', 'is_late', 'days_late']].head())
```

::: {.callout-tip}
## üí° Method Chaining Tip for New Python Users

**Why use `lambda df:`?** When chaining methods, we need to reference the current state of the dataframe. The `lambda df:` tells pandas "use the current dataframe in this calculation." Without it, pandas would look for a variable called `df` that doesn't exist.

**Alternative approach:** You could also write this as separate steps, but method chaining keeps related operations together and makes the code more readable.
:::

::: {.callout-important}
## ü§î Discussion Questions: Assign Mental Model

**Question 1: Data Types and Date Handling**
- What is the `dtype` of the `actualShipDate` series? How can you find out using code?
- Why is it important that both `actualShipDate` and `plannedShipDate` have the same data type for comparison?

**Question 2: String vs Date Comparison**
- Can you give an example where comparing two dates as strings would yield unintuitive results, e.g. what happens if you try to compare "04-11-2025" and "05-20-2024" as strings vs as dates?

**Question 3: Debug This Code**
```{python}
# This code has an error - can you spot it?
shipments_with_lateness = (
    shipments_df
    .assign(
        is_late=lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late=lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days,
        #lateStatement="Darn Shipment is Late" if shipments_df['is_late'] else "Shipment is on Time"
        lateStatement=lambda df: np.where(df['is_late'], "Darn Shipment is Late", "Shipment is on Time")
    )
)
```
What's wrong with the `lateStatement` assignment and how would you fix it?
:::

::: {.callout-tip}
## Your Answer: Data Types and Date Handling
*<M8[us] > datetime64 - shipments_df['actualShipDate'].dtype*
*Being the same data type allows cross comparisons of the dates. If the dates are not the same data type, the comparison will not work and will return an error.*
:::

::: {.callout-tip}
## Your Answer: String vs Date Comparison
*When comparing two dates as strings, the comparison will be based on the first character of the string. For example, "04-11-2025" is smaller than "05-20-2024" because the first character of "04-11-2025" is "04" which is smaller than the first character of "05-20-2024" which is "05".*
*When comparing two dates as dates, the comparison will be based on the actual date values.So "04-11-2025" is considered larger than "05-20-2024" because the the year 2025 is after or larger than the year 2024.*
:::

::: {.callout-tip}
## Your Answer: Debug This Code
*is_late is being created in the same .assign() call, so it doesn‚Äôt exist on shipments_df yet.lateStatement is not inside a lambda, so it doesn‚Äôt receive the updated dataframe df that would contain is_late*
:::

### 2. Subset: Querying Rows and Filtering Columns

**Mental Model:** Query rows based on conditions and filter to keep specific columns.

Let's query for only late shipments and filter to keep the columns we need:

```{python}
#| label: mental-model-2-subset
#| echo: true

# Query rows for late shipments and filter to keep specific columns
late_shipments = (
    shipments_with_lateness
    .query('is_late == True')  # Query rows where is_late is True
    .filter(['shipID', 'partID', 'plannedShipDate', 'actualShipDate', 'days_late'])  # Filter to keep specific columns
)

print(f"Found {len(late_shipments)} late shipments out of {len(shipments_with_lateness)} total")
print("\nLate shipments sample:")
print(late_shipments.head())
```

::: {.callout-note}
## üîç Understanding the Methods

- **`.query()`**: Query rows based on conditions (like SQL WHERE clause)
- **`.filter()`**: Filter to keep specific columns by name
- **Alternative**: You could use `.loc[]` for more complex row querying, but `.query()` is often more readable
:::

::: {.callout-important}
## ü§î Discussion Questions: Subset Mental Model

**Question 1: Query vs Boolean Indexing**
- What's the difference between using `.query('is_late == True')` and `[df['is_late'] == True]`?
- Which approach is more readable and why?

**Question 2: Additional Row Querying**
- Can you show an example of using a variable like `late_threshold` to query rows for shipments that are at least `late_threshold` days late, e.g. what if you wanted to query rows for shipments that are at least 5 days late?
:::

::: {.callout-tip}
## Your Answer: Query vs Boolean Indexing
*The main differences are syntax  and how you refer to columns (names in a string vs df['col']). For this single condition, the resulting subset is the same but when there are multiple conditions,the query syntax is more readable and easier to understand. Because query is more like English and has less repetitive syntax to write and for people to read which is easier to understand and write.*
:::

::: {.callout-tip}
## Your Answer: Additional Row Querying
```{python}
late_threshold = 5
very_late_shipments = (
    shipments_with_lateness
    .query('days_late >= @late_threshold')
    .filter(['shipID', 'partID', 'plannedShipDate', 'actualShipDate', 'days_late'])
)
print(f"Shipments at least {late_threshold} days late: {len(very_late_shipments)}")
print(very_late_shipments.head())
```
*Use a variable (e.g. `late_threshold = 5`) and refer to it inside `.query()` with `@`:`df.query('days_late >= @late_threshold')`. The `@` tells pandas to use the Python variable's value, not a column name. Example: `late_threshold = 5` then `shipments_with_lateness.query('days_late >= @late_threshold')` gives rows that are at least 5 days late. Change `late_threshold` to reuse the same query for different cutoffs days.*
:::

### 3. Drop: Removing Unwanted Data

**Mental Model:** Remove columns or rows you don't need.

Let's clean up our data by removing unnecessary columns:

```{python}
#| label: mental-model-3-drop
#| echo: true

# Create a cleaner dataset by dropping unnecessary columns
clean_shipments = (
    shipments_with_lateness
    .drop(columns=['quantity'])  # Drop quantity column (not needed for our analysis)
    .dropna(subset=['plannedShipDate', 'actualShipDate'])  # Remove rows with missing dates
)

print(f"Cleaned dataset: {len(clean_shipments)} rows, {len(clean_shipments.columns)} columns")
print("Remaining columns:", clean_shipments.columns.tolist())
```

::: {.callout-important}
## ü§î Discussion Questions: Drop Mental Model

**Question 1: Drop vs Filter Strategies**
- What's the difference between `.drop(columns=['quantity'])` and `.filter()` with a list of columns you want to keep?
- When would you choose to drop columns vs filter to keep specific columns?

**Question 2: Handling Missing Data**
- What happens if you use `.dropna()` without specifying `subset`? How is this different from `.dropna(subset=['plannedShipDate', 'actualShipDate'])`?
- Why might you want to be selective about which columns to check for missing values?

:::

::: {.callout-tip}
## Your Answer: Drop vs Filter Strategies
*Difference: `.drop(columns=['quantity'])` specifies which columns to remove ‚Äî you get all columns except the ones you drop. `.filter([...])` with a list of column names specifies which columns to keep ‚Äî you get only those columns.*

*When to choose which: Use drop when you want to remove a small number of columns from a wide table (e.g. drop one or two you don‚Äôt need). Use filter (or column selection) when you want a specific subset of columns and the rest don‚Äôt matter ‚Äî e.g. for reporting or passing data to a function that expects certain columns. Drop is ‚Äúremove these‚Äù; filter is ‚Äúkeep only these.‚Äù*
:::

::: {.callout-tip}
## Your Answer: Handling Missing Data
*Without `subset`: `.dropna()` drops every row that has a missing value in any column. One NaN in any column removes that row.*

*With `subset`: `.dropna(subset=['plannedShipDate', 'actualShipDate'])` drops only rows that are missing in those columns. Other columns can have NaNs and the row is kept.*

*Why be selective? You often care about missingness only in columns that matter for your analysis (e.g. dates for lateness). If other columns have valid NaNs or optional data, using `.dropna()` on the whole table would remove too many rows. Being selective keeps rows where the key columns are present and ignores missing values in less important columns.*
:::

### 4. Sort: Arranging Data

**Mental Model:** Order data by values or indices.

Let's sort by lateness to see the worst offenders:

```{python}
#| label: mental-model-4-sort
#| echo: true

# Sort by days late (worst first)
sorted_by_lateness = (
    clean_shipments
    .sort_values('days_late', ascending=False)  # Sort by days_late, highest first
    .reset_index(drop=True)  # Reset index to be sequential
)

print("Shipments sorted by lateness (worst first):")
print(sorted_by_lateness[['shipID', 'partID', 'days_late', 'is_late']].head(10))
```

::: {.callout-important}
## ü§î Discussion Questions: Sort Mental Model

**Question 1: Sorting Strategies**
- What's the difference between `ascending=False` and `ascending=True` in sorting?
- How would you sort by multiple columns (e.g., first by `is_late`, then by `days_late`)?

**Question 2: Index Management**
- Why do we use `.reset_index(drop=True)` after sorting?
- What happens to the original index when you sort? Why might this be problematic?

:::

::: {.callout-tip}
## Your Answer: Sorting Strategies
*`ascending=False`will sort in descending order and `ascending=True` will sort in ascending order.*

*df.sort_values(by=['is_late', 'days_late'], ascending=[False, True]) will sort by is_late in descending order and then by days_late in ascending order.*
:::

::: {.callout-tip}
## Your Answer: Index Management
*If we don't use .reset_index(drop=True), the index will be the original index which the first row in the sorted data may not be the first row in the original data. We use .reset_index(drop=True) to reset the index to be sequential and drop/delete the original index.*

*When you sort, rows are reordered but their index labels stay attached. So the index does not reflect the new order. Which can lead to gaps and duplicates where you can end up with non-sequential indices like [5, 2, 8, 1]. It may also cause problems for future operations where functions that assume a regular 0-based index can behave unexpectedly.*
:::

### 5. Aggregate: Summarizing Data

**Mental Model:** Calculate summary statistics across groups or the entire dataset.

Let's calculate overall service level metrics:

```{python}
#| label: mental-model-5-aggregate
#| echo: true

# Calculate overall service level metrics
service_metrics = (
    clean_shipments
    .agg({
        'is_late': ['count', 'sum', 'mean'],  # Count total, count late, calculate percentage
        'days_late': ['mean', 'max']  # Average and maximum days late
    })
    .round(3)
)

print("Overall Service Level Metrics:")
print(service_metrics)

# Calculate percentage on-time directly from the data
on_time_rate = (1 - clean_shipments['is_late'].mean()) * 100
print(f"\nOn-time delivery rate: {on_time_rate:.1f}%")
```

::: {.callout-important}
## ü§î Discussion Questions: Aggregate Mental Model

**Question 1: Boolean Aggregation**
- Why does `sum()` work on boolean values? What does it count?

:::

::: {.callout-tip}
## Your Answer: Boolean Aggregation
*Boolean values in python are integers which are True = 1 and False = 0. So sum() will count/add up the number of True values.*
:::

### 6. Merge: Combining Information

**Mental Model:** Join data from multiple sources to create richer datasets.

Now let's analyze service levels by product category. First, we need to merge our data:

```{python}
#| label: mental-model-6-merge-prep
#| echo: true

# Merge shipment data with product line data
shipments_with_category = (
    clean_shipments
    .merge(product_line_df, on='partID', how='left')  # Left join to keep all shipments
    .assign(
        category_late=lambda df: df['is_late'] & df['prodCategory'].notna()  # Only count as late if we have category info
    )
)

print("\nProduct categories available:")
print(shipments_with_category['prodCategory'].value_counts())
```

::: {.callout-important}
## ü§î Discussion Questions: Merge Mental Model

**Question 1: Join Types and Data Loss**
- Why does your professor think we should use `how='left'` in most cases? 
- How can you check if any shipments were lost during the merge?

**Question 2: Key Column Matching**
- What happens if there are duplicate `partID` values in the `product_line_df`?

:::

::: {.callout-tip}
## Your Answer: Join Types and Data Loss
*'how=left' will add all new columns to the right side of the existing columns. A left join keeps every row in clean_shipments, regardless of whether there is a matching partID in product_line_df.*
*With how='inner', any shipment whose partID is not in product_line_df would be dropped. That can remove data without a clear signal.*
*. Non-matching rows get NaNs in the new columns instead of disappearing.*
:::

::: {.callout-tip}
## Your Answer: Key Column Matching
*A left merge on partID will keep all rows from clean_shipments, but for each row in clean_shipments, every matching row in product_line_df is joined. If product_line_df has duplicates on partID, this causes one-to-many expansion:*
*For a single shipID (and partID), you get multiple rows in the merged result‚Äîone for each matching row in product_line_df.*
*Total row count in shipments_with_category becomes larger than clean_shipments.*
*Fields from product_line_df (e.g. prodCategory) can differ across those rows if the duplicate partID rows have different values.*
:::

### 7. Split-Apply-Combine: Group Analysis

**Mental Model:** Group data and apply functions within each group.

Now let's analyze service levels by category:

```{python}
#| label: mental-model-7-groupby
#| echo: true

# Analyze service levels by product category
service_by_category = (
    shipments_with_category
    .groupby('prodCategory')  # Split by product category
    .agg({
        'is_late': ['any', 'count', 'sum', 'mean'],  # Count, late count, percentage late
        'days_late': ['mean', 'max']  # Average and max days late
    })
    .round(3)
)

print("Service Level by Product Category:")
print(service_by_category)
```

::: {.callout-important}
## ü§î Discussion Questions: Split-Apply-Combine Mental Model

**Question 1: GroupBy Mechanics**
- What does `.groupby('prodCategory')` actually do? How does it "split" the data?
- Why do we need to use `.agg()` after grouping? What happens if you don't?

**Question 2: Multi-Level Grouping**
- Explore grouping by `['shipID', 'prodCategory']`?  What question does this answer versus grouping by `'prodCategory'` alone?  (HINT: There may be many rows with identical shipID's due to a particular order having multiple partID's.)
:::

::: {.callout-tip}
## Your Answer: GroupBy Mechanics
*.groupby('prodCategory') builds a GroupBy object that conceptually splits the DataFrame into subgroups by distinct values of prodCategory. For example, with prodCategory values like "Electronics", "Clothing", "Furniture", you get one group per category.The result is not a DataFrame but a lazy grouping structure. It doesn‚Äôt compute anything until you call a method like .sum(), .mean(), .agg(), etc.*
*If we use .agg(): we explicitly choose the aggregation(s), e.g.grouped.agg({'price': 'mean', 'quantity': 'sum'})If we don‚Äôt aggregate at all after groupby, we end up with a GroupBy object that hasn‚Äôt been reduced. We can iterate over it or call other methods, but we won‚Äôt get a summary table until we perform some aggregation.*
:::

::: {.callout-tip}
## Your Answer: Multi-Level Grouping
*Grouping by ['shipID', 'prodCategory'] answers question like: ‚ÄúFor each shipment (order), what is the total (or average, etc.) for each product category within that order?‚Äù We get one row per combination of shipID and prodCategory. We get one row per combination of shipID and prodCategory. Because one order can have multiple lines (different partIDs) in the same category, grouping by ['shipID', 'prodCategory'] aggregates those multiple lines into a single row per order per category. For example, if order #123 has three electronics parts, those three rows become one row for that order and category, with quantities/prices summed or averaged as we specify.*
*Grouping by 'prodCategory' alone answers question like: ‚ÄúAcross all orders, what is the total (or average, etc.) for each product category?‚Äù We get one summary row per category for the whole dataset. All orders are combined, so we don‚Äôt see how each shipment contributes.*
:::

## Answering A Business Question

**Putting It All Together:** Combine multiple data manipulation techniques to answer complex business questions.

Let's create a comprehensive analysis by combining shipment-level data with category information:

```{python}
#| label: mental-model-7-comprehensive
#| echo: true

# Create a comprehensive analysis dataset
comprehensive_analysis = (
    shipments_with_category
    .groupby(['shipID', 'prodCategory'])  # Group by shipment and category
    .agg({
        'is_late': 'any',  # True if any item in this shipment/category is late
        'days_late': 'max'  # Maximum days late for this shipment/category
    })
    .reset_index()
    .assign(
        has_multiple_categories=lambda df: df.groupby('shipID')['prodCategory'].transform('nunique') > 1
    )
)

print("Comprehensive analysis - shipments with multiple categories:")
multi_category_shipments = comprehensive_analysis[comprehensive_analysis['has_multiple_categories']]
print(f"Shipments with multiple categories: {multi_category_shipments['shipID'].nunique()}")
print(f"Total unique shipments: {comprehensive_analysis['shipID'].nunique()}")
print(f"Percentage with multiple categories: {multi_category_shipments['shipID'].nunique() / comprehensive_analysis['shipID'].nunique() * 100:.1f}%")
```

::: {.callout-important}
## ü§î Discussion Questions: Answering A Business Question

**Question 1: Business Question Analysis**
- What business question does this comprehensive analysis answer?
- How does grouping by `['shipID', 'prodCategory']` differ from grouping by just `'prodCategory'`?
- What insights can ZappTech's management gain from knowing the percentage of multi-category shipments?
:::

::: {.callout-tip}
## Your Answer: Business Question Analysis
*1. What business question does this comprehensive analysis answer?*

*It answers: ‚ÄúHow do late shipments relate to product category mix, and how common are multi-category shipments?‚Äù It can let ZappTech see whether late shipments span multiple categories, where delays are worst by category, and how often shipments include more than one product category.*

*2. How does grouping by `['shipID', 'prodCategory']` differ from grouping by just `'prodCategory'`?*

*['shipID', 'prodCategory']: One row per shipment‚Äìcategory pair. Each shipment can appear multiple times if it has items from different categories. This keeps the shipment-level detail needed for multi-category analysis.*
*['prodCategory']: One row per category across all shipments. Shipment identity is lost, so you can‚Äôt measure how many shipments mix categories or study late shipments by category within each shipment.*

*3. What insights can ZappTech's management gain from knowing the percentage of multi-category shipments?* 
*Supply chain: High multi-category share suggests orders often mix categories, which may increase coordination, picking, packing, or routing complexity.*
*Operations: Multi-category shipments may have different delay patterns than single-category ones; this supports more targeted interventions.*
*Customer behavior: Reflects how often customers order across categories in one shipment, which informs bundling, promotions, and fulfillment design.*
*Capacity planning: Helps estimate how often warehouses handle mixed loads vs single-category loads, which affects staffing and layout.*
:::

## Student Analysis Section: Mastering Data Manipulation {#student-analysis-section}

**Your Task:** Demonstrate your mastery of the seven mental models through comprehensive discussion and analysis. The bulk of your grade comes from thoughtfully answering the discussion questions for each mental model. See below for more details.

### Core Challenge: Discussion Questions Analysis

**For each mental model, provide:**
- Clear, concise answers to all discussion questions
- Code examples where appropriate to support your explanations

::: {.callout-important}
## üìä Discussion Questions Requirements

**Complete all discussion question sections:**
1. **Assign Mental Model:** Data types, date handling, and debugging
2. **Subset Mental Model:** Filtering strategies and complex queries
3. **Drop Mental Model:** Data cleaning and quality management
4. **Sort Mental Model:** Data organization and business logic
5. **Aggregate Mental Model:** Summary statistics and business metrics
6. **Merge Mental Model:** Data integration and quality control
7. **Split-Apply-Combine Mental Model:** Group analysis and advanced operations
8. **Answering A Business Question:** Combining multiple data manipulation techniques to answer a business question
:::

### Professional Visualizations (For 100% Grade)

**Your Task:** Create a professional visualization that supports your analysis and demonstrates your understanding of the data.

**Create visualizations showing:**
- Service level (on-time percentage) by product category

**Your visualizations should:**
- Use clear labels and professional formatting
- Support the insights from your discussion questions
- Be appropriate for a business audience
- Do not `echo` the code that creates the visualizations

```{python}
#| label: viz-service-level
#| echo: false
#| fig-cap: "Service Level by Product Category ‚Äî On-time percentage of shipment-category combinations across ZappTech's product categories."
#| fig-align: center

# Calculate on-time percentage (service level) and shipment count by product category
on_time_by_category = (
    comprehensive_analysis
    .groupby('prodCategory')
    .agg(
        on_time_pct=('is_late', lambda x: (1 - x.mean()) * 100),
        n=('is_late', 'count')
    )
    .reset_index()
    .sort_values('on_time_pct')
)


sns.set_theme(style='whitegrid', font_scale=1.05, rc={'axes.facecolor': '#f8f9fa', 'figure.facecolor': 'white'})
fig, ax = plt.subplots(figsize=(10, 6), dpi=120)

colors = ['#27ae60' if pct >= 90 else '#2980b9' if pct >= 80 else '#c0392b' for pct in on_time_by_category['on_time_pct']]
bars = ax.barh(
    on_time_by_category['prodCategory'],
    on_time_by_category['on_time_pct'],
    color=colors,
    edgecolor='white',
    linewidth=1.2,
    height=0.72
)

ax.axvline(x=90, color='#7f8c8d', linestyle='--', linewidth=2, alpha=0.9, label='90% Target')
ax.set_xlabel('On-Time Percentage (Service Level) (%)', fontsize=11, fontweight='600')
ax.set_ylabel('Product Category', fontsize=11, fontweight='600')
ax.set_title('Service Level by Product Category\nZappTech Shipment Performance', fontsize=13, fontweight='bold', pad=12)
ax.set_xlim(0, 105)
ax.legend(loc='lower right', frameon=True, fontsize=9)

for bar, pct, n in zip(bars, on_time_by_category['on_time_pct'], on_time_by_category['n']):
    x_pos = bar.get_width() + 2 if bar.get_width() < 85 else bar.get_width() - 8
    ha = 'left' if bar.get_width() < 85 else 'right'
    text_color = '#2c3e50' if bar.get_width() < 85 else 'white'
    ax.text(x_pos, bar.get_y() + bar.get_height()/2, f'{pct:.1f}%  (n={n})', va='center', ha=ha, fontsize=9, fontweight='500', color=text_color)

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
fig
```

## Grading Rubric üéì {#grading-rubric}

**75% Grade:** Complete discussion questions for at least 5 of the 7 mental models with clear, thoughtful responses.

**85% Grade:** Complete discussion questions for all 7 mental models with comprehensive, well-reasoned responses.

**95% Grade:** Complete all discussion questions plus the "Answering A Business Question" section.

**100% Grade:** Complete all discussion questions plus create a professional visualization showing service level by product category.

## Submission Checklist ‚úÖ

**Setup & Deployment (Required for Any Points):**

- [ ] Forked repository from [flyaflya/dataManipulationChallenge](https://github.com/flyaflya/dataManipulationChallenge) to your GitHub account
- [ ] Cloned **your fork** locally using Cursor (or VS Code)
- [ ] Document rendered to HTML successfully
- [ ] HTML files pushed to your repository
- [ ] GitHub Pages enabled and working
- [ ] Site accessible at `https://[your-username].github.io/dataManipulationChallenge/`

**Content (See [Grading Rubric](#grading-rubric) for Grade Tiers):**

- [ ] Discussion questions completed for at least 5 of 7 mental models (75%)
- [ ] Discussion questions completed for all 7 mental models (85%)
- [ ] "Answering A Business Question" discussion questions completed (95%)
- [ ] Professional visualization showing service level by product category (100%)

**Report Quality (Critical for Higher Grades):**

- [ ] Professional writing style (no AI-generated fluff)
- [ ] Concise analysis that gets to the point
- [ ] Code examples where appropriate to support explanations


